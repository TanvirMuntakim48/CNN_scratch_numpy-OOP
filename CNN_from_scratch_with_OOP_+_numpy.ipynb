{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of CNN from scratch using OOP and numpy\n",
        "\n",
        "In this notebook, I implement CNN using numpy only and declare these layers in terms of classes, writing forward and backward functions inside these classes. This is much more compact than writing the forward and backward equations in separate functions and takes much less lines to write.\n",
        "\n",
        "This method of coding is learned from \"The Independent Code\" Channel in youtube from their playlist of writing neural networks from scratch: https://www.youtube.com/@independentcode"
      ],
      "metadata": {
        "id": "C-gSHtlN22M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Layer\n",
        "\n",
        "Two attributes. Declared for convenience so other layers don't have to declare them:\n",
        "\n",
        "* `self.forward()` - return layer output\n",
        "* `self.backward()` - update parameters of the layer and return input gradient. We can pass an optimizer here as inputs but, for simplicity, we will pass only the learning rate here to update via Gradient Descent\n",
        "\n"
      ],
      "metadata": {
        "id": "TWFAOviO5w6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1, 2, 3, -1, -2, 1], [-4, -3, 2, 13, -1, 5]])\n",
        "np.maximum(a, 0)\n",
        "(a > 0).astype(\"float32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dhJzPS4Y8L_",
        "outputId": "dcf342f4-c864-433d-ee25-ba4dd8765fd9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 0., 0., 1.],\n",
              "       [0., 0., 1., 1., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: return output\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # TODO: update parameters and return input gradient\n",
        "        pass"
      ],
      "metadata": {
        "id": "18FNHKP05z-1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dense Layer\n",
        "\n",
        "Will inherit from `Layer()` class and we will define it's own `self.forward()` and `self.backward()` methods.\n",
        "\n",
        "This layer will behave like a Linear Layer (without activation)"
      ],
      "metadata": {
        "id": "yMazfyO37ARH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2ax66DUf5mzg"
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size)\n",
        "        self.bias = np.random.randn(output_size, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(self.weights, self.input) + self.bias\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        return input_gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Layer\n",
        "\n",
        "Inherits from base Layer class. Applies non-linear activation on inputs. Also has forward and backward methods.\n",
        "\n",
        "Also defined Tanh() activation that performs the tanh operation on inputs."
      ],
      "metadata": {
        "id": "gcr1TOaq7YV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        tanh = lambda x: np.tanh(x)\n",
        "        tanh_prime = lambda x: 1 - np.tanh(x) ** 2\n",
        "        super().__init__(tanh, tanh_prime)"
      ],
      "metadata": {
        "id": "fruJ-Z6a7X3U"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import signal\n",
        "\n",
        "#\n",
        "class Convolutional(Layer):\n",
        "    def __init__(self, input_shape, kernel_size, depth):\n",
        "        # unpacking the input shape\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "\n",
        "        # storing the depth of the kernel\n",
        "        self.depth = depth\n",
        "\n",
        "        # storing the input shape\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        # storing the input depth\n",
        "        self.input_depth = input_depth\n",
        "\n",
        "        # Creating and storing the output shape\n",
        "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
        "\n",
        "        # Initializing the parameters of the layer, i.e, the kernels and biases\n",
        "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "        self.kernels = np.random.randn(*self.kernels_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = np.copy(self.biases)\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        kernels_gradient = np.zeros(self.kernels_shape)\n",
        "        input_gradient = np.zeros(self.input_shape)\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
        "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
        "\n",
        "        self.kernels -= learning_rate * kernels_gradient\n",
        "        self.biases -= learning_rate * output_gradient\n",
        "        return input_gradient"
      ],
      "metadata": {
        "id": "lx2leThZ5_QE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshape(Layer):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, self.output_shape)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.reshape(output_gradient, self.input_shape)"
      ],
      "metadata": {
        "id": "IoK0wfIRCLPr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_prime(y_true, y_pred):\n",
        "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
      ],
      "metadata": {
        "id": "WZunE8B98Hs4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "        sigmoid_prime = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
        "        super().__init__(sigmoid, sigmoid_prime)"
      ],
      "metadata": {
        "id": "6SGfktHaDw5E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBg8KsXvGjR4",
        "outputId": "44039010-2939-438a-bd1e-689b06059fdc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: np_utils in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#\n",
        "def preprocess_data(x, y, limit):\n",
        "    zero_index = np.where(y == 0)[0][:limit]\n",
        "    one_index = np.where(y == 1)[0][:limit]\n",
        "    all_indices = np.hstack((zero_index, one_index))\n",
        "    all_indices = np.random.permutation(all_indices)\n",
        "    x, y = x[all_indices], y[all_indices]\n",
        "    x = x.reshape(len(x), 1, 28, 28)\n",
        "    x = x.astype(\"float32\") / 255\n",
        "    y = to_categorical(y)\n",
        "    y = y.reshape(len(y), 2, 1)\n",
        "    return x, y\n",
        "\n",
        "# load MNIST from server, limit to 100 images per class since we're not training on GPU\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
        "x_test, y_test = preprocess_data(x_test, y_test, 100)\n",
        "\n",
        "# neural network\n",
        "network = [\n",
        "    Convolutional((1, 28, 28), 3, 5),\n",
        "    Sigmoid(),\n",
        "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
        "    Dense(5 * 26 * 26, 100),\n",
        "    Sigmoid(),\n",
        "    Dense(100, 2),\n",
        "    Sigmoid()\n",
        "]\n",
        "#\n",
        "epochs = 20\n",
        "learning_rate = 0.1\n",
        "#\n",
        "# train\n",
        "for e in range(epochs):\n",
        "    error = 0\n",
        "    for x, y in zip(x_train, y_train):\n",
        "        # forward\n",
        "        output = x\n",
        "        for layer in network:\n",
        "            output = layer.forward(output)\n",
        "\n",
        "        # error\n",
        "        error += binary_cross_entropy(y, output)\n",
        "\n",
        "        # backward\n",
        "        grad = binary_cross_entropy_prime(y, output)\n",
        "        for layer in reversed(network):\n",
        "            grad = layer.backward(grad, learning_rate)\n",
        "\n",
        "    error /= len(x_train)\n",
        "    print(f\"{e + 1}/{epochs}, error={error}\")\n",
        "#\n",
        "# test\n",
        "for x, y in zip(x_test, y_test):\n",
        "    output = x\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "    print(f\"pred: {np.argmax(output)}, true: {np.argmax(y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jiuPF6o8Jnt",
        "outputId": "9e4159e7-4aa3-4660-eea7-8f15164a4a0c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/20, error=0.7172536206640002\n",
            "2/20, error=0.22676229512963794\n",
            "3/20, error=0.09435768043383909\n",
            "4/20, error=0.07071678348019511\n",
            "5/20, error=0.04303018625549971\n",
            "6/20, error=0.03128314947147966\n",
            "7/20, error=0.02333603634568812\n",
            "8/20, error=0.01640286146176707\n",
            "9/20, error=0.012327391875588954\n",
            "10/20, error=0.009713587990739753\n",
            "11/20, error=0.008478706182171728\n",
            "12/20, error=0.007239537103126336\n",
            "13/20, error=0.0063792193595302886\n",
            "14/20, error=0.005709957481155583\n",
            "15/20, error=0.005168291911871626\n",
            "16/20, error=0.004725594338428851\n",
            "17/20, error=0.004353750460964956\n",
            "18/20, error=0.004032603321003528\n",
            "19/20, error=0.003749673042518586\n",
            "20/20, error=0.0035011335383866517\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 1, true: 1\n",
            "pred: 0, true: 0\n",
            "pred: 0, true: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzcmh69j8KHC"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}